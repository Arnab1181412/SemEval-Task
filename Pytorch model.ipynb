{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09e43aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import spacy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torch.optim as optim\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff20f610",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path = 'TRAIN_FILE.TXT'\n",
    "with open(training_path,'r') as f:\n",
    "    text = f.readlines()    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3334eb37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the system as described above has its greatest application in an arrayed <e1>configuration</e1> of antenna <e2>elements</e2>.',\n",
       " 'the <e1>child</e1> was carefully wrapped and bound into the <e2>cradle</e2> by means of a cord.',\n",
       " 'the <e1>author</e1> of a keygen uses a <e2>disassembler</e2> to look at the raw assembly code.',\n",
       " 'a misty <e1>ridge</e1> uprises from the <e2>surge</e2>.',\n",
       " 'the <e1>student</e1> <e2>association</e2> is the voice of the undergraduate student population of the state university of new york at buffalo.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = []\n",
    "for index in range(0,8000,4):\n",
    "    text[index] = text[index].replace('\\t',' ')\n",
    "    text[index] = text[index].replace('\\\"','')\n",
    "    text[index] = text[index].replace('\\n','')\n",
    "    words = text[index].split()\n",
    "    text[index] = ' '.join(words[1:])\n",
    "    \n",
    "    sentences.append(text[index].lower())\n",
    "sentences[:5]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3edd5897",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-3161dcca0716>:40: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:210.)\n",
      "  pos_entity = torch.tensor(pos_entity)\n"
     ]
    }
   ],
   "source": [
    "pos_entity = []\n",
    "for i,sentence in enumerate(sentences):\n",
    "    \n",
    "    pos_e1_low = sentence.index('<e1>') + 4\n",
    "    pos_e1_high = sentence.index('</e1>')\n",
    "    e1 = sentence[pos_e1_low:pos_e1_high]\n",
    "    \n",
    "    pos_e2_low = sentence.index('<e2>') + 4\n",
    "    pos_e2_high = sentence.index('</e2>')\n",
    "    e2 = sentence[pos_e2_low:pos_e2_high]\n",
    "    \n",
    "    sentences[i] = re.sub('<e1>|</e1>|<e2>|</e2>',' ',sentence)\n",
    "    tokens = sentences[i].split()\n",
    "    sentences[i] = ' '.join(tokens)\n",
    "#     print(tokens)\n",
    "    e1_list = []\n",
    "    for entity in e1.split():\n",
    "        e1_list.append(tokens.index(entity))\n",
    "    e2_list = []\n",
    "    for entity in e2.split():\n",
    "        e2_list.append(tokens.index(entity))\n",
    "    \n",
    "    pos_entity.append([e1_list,e2_list])\n",
    "max_tokens = 0\n",
    "for element in pos_entity:\n",
    "#     print(element)\n",
    "    e1 = len(element[0])\n",
    "    e2 = len(element[1])\n",
    "    local_max = max(e1,e2)\n",
    "    max_tokens = max(max_tokens,local_max)\n",
    "for i,element in enumerate(pos_entity):\n",
    "    e1_len = max_tokens - len(element[0])\n",
    "    e1 = torch.tensor(element[0])\n",
    "    e1 = F.pad(e1,(0,e1_len),\"constant\",-1).numpy()\n",
    "    \n",
    "    e2_len = max_tokens - len(element[1])\n",
    "    e2 = torch.tensor(element[1])\n",
    "    e2 = F.pad(e2,(0,e2_len),\"constant\",-1).numpy()\n",
    "    pos_entity[i] = np.array([e1,e2])\n",
    "pos_entity = torch.tensor(pos_entity)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73673a58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Component-Whole(e2,e1)': 111,\n",
       "         'Other-Relation': 310,\n",
       "         'Instrument-Agency(e2,e1)': 109,\n",
       "         'Member-Collection(e1,e2)': 22,\n",
       "         'Cause-Effect(e2,e1)': 157,\n",
       "         'Entity-Destination(e1,e2)': 238,\n",
       "         'Content-Container(e1,e2)': 117,\n",
       "         'Message-Topic(e1,e2)': 130,\n",
       "         'Product-Producer(e2,e1)': 87,\n",
       "         'Member-Collection(e2,e1)': 174,\n",
       "         'Entity-Origin(e1,e2)': 144,\n",
       "         'Cause-Effect(e1,e2)': 94,\n",
       "         'Component-Whole(e1,e2)': 114,\n",
       "         'Message-Topic(e2,e1)': 44,\n",
       "         'Product-Producer(e1,e2)': 66,\n",
       "         'Entity-Origin(e2,e1)': 28,\n",
       "         'Content-Container(e2,e1)': 30,\n",
       "         'Instrument-Agency(e1,e2)': 25})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allowed_relations = ['Cause-Effect', 'Component-Whole', 'Entity-Destination','Entity-Origin','Other-Relation',\n",
    "                    'Instrument-Agency','Member-Collection','Content-Container','Message-Topic','Product-Producer']\n",
    "relations = []\n",
    "for index in range(1,8000,4):\n",
    "    text[index] = text[index].replace('\\n','')\n",
    "    if text[index].find('(') != -1:\n",
    "        index_of_braces = text[index].find('(')\n",
    "        relation = text[index][:index_of_braces]\n",
    "        if relation not in allowed_relations:\n",
    "            relation = 'Other-Relation'\n",
    "            text[index] = relation + text[index][index_of_braces:]\n",
    "    else:\n",
    "        text[index] = 'Other-Relation'\n",
    "    relations.append(text[index])\n",
    "Counter(relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86cfb9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "input_dataset = []\n",
    "max_len = max([len(sentence.split()) for sentence in sentences])\n",
    "for sentence in sentences:\n",
    "    doc = nlp(sentence)\n",
    "    vector = []\n",
    "    for token in doc:\n",
    "        vector.append(token.vector)\n",
    "    vector = torch.tensor(vector)\n",
    "    right_padding = max_len-vector.shape[0]\n",
    "    padding = nn.ZeroPad2d((0,right_padding,0,0))\n",
    "    vector = padding(vector.T)\n",
    "    input_dataset.append(vector.T.numpy())\n",
    "    \n",
    "input_dataset = torch.tensor(np.array(input_dataset),device='cuda:0')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa13b9a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2000, 10])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "just_relation = []\n",
    "for relation in relations:\n",
    "    if relation.find('(')!=-1:\n",
    "        index = relation.find(\"(\")\n",
    "        relation = relation[:index]\n",
    "        just_relation.append(relation)\n",
    "    else:\n",
    "        just_relation.append(relation)\n",
    "just_relation = np.array(just_relation)        \n",
    "encoder = LabelEncoder()\n",
    "just_relation = torch.tensor(encoder.fit_transform(just_relation))\n",
    "ohe = F.one_hot(just_relation)\n",
    "ohe = ohe.to(dtype=torch.float)\n",
    "ohe.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "344c3e23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([251., 225., 147., 238., 172., 134., 196., 174., 310., 153.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df82c618",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data = input_dataset[:1600]\n",
    "        self.target = ohe[:1600]\n",
    "        self.position = pos_entity[:1600]\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        sample = self.data[index],self.position[index],self.target[index]\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data = input_dataset[1600:]\n",
    "        self.target = ohe[1600:]\n",
    "        self.position = pos_entity[1600:]\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        sample = self.data[index],self.position[index],self.target[index]\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d91dd826",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = TrainDataset()\n",
    "train_dataloader = DataLoader(train,batch_size=128,shuffle=True)\n",
    "test = TestDataset()\n",
    "test_dataloader = DataLoader(test,batch_size=128,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7631363f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self,input_shape,lstm_hidden_size,num_layers,output_shape,device):\n",
    "        super(MyModel,self).__init__()\n",
    "        self.input_shape = input_shape \n",
    "        self.hidden_size = lstm_hidden_size\n",
    "        self.output_shape = output_shape\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "        self.bidirectional_lstm = nn.LSTM(input_shape,lstm_hidden_size,num_layers,\n",
    "                                          bidirectional=True,batch_first=True)\n",
    "        E = self.hidden_size\n",
    "        w = torch.empty(E,E)\n",
    "        self.query_weights = nn.init.xavier_uniform_(w).to(device)\n",
    "        self.key_weights = nn.init.xavier_uniform_(w).to(device)\n",
    "        self.value_weights = nn.init.xavier_uniform_(w).to(device)\n",
    "        self.attention = nn.MultiheadAttention(lstm_hidden_size,num_heads=1,batch_first=True)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(2*lstm_hidden_size,output_shape)\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        h0,c0 = self.initialize_states(inputs[0].shape[0])\n",
    "        lstm_outputs,_ = self.bidirectional_lstm(inputs[0],(h0,c0))\n",
    "        lstm_outputs = torch.mean(lstm_outputs.view(lstm_outputs.shape[0],-1,2,self.hidden_size),dim=2)\n",
    "        query,key,value = self.calculate_qkv(lstm_outputs)\n",
    "        attn_outputs,_ = self.attention(query,key,value)\n",
    "        \n",
    "        batch_size = attn_outputs.shape[0]\n",
    "        linear_inputs = torch.empty(batch_size,2,attn_outputs.shape[-1])\n",
    "#         print(attn_outputs.shape)\n",
    "        for i in range(batch_size):\n",
    "            l1 = []\n",
    "            for j in inputs[1][i][0]:\n",
    "                if j != -1:\n",
    "                    l1.append(j.item())\n",
    "            l2 = []\n",
    "            for k in inputs[1][i][1]:\n",
    "                if k != -1:\n",
    "                    l2.append(k.item())\n",
    "#             print(attn_outputs[i][l1].shape)\n",
    "            entity_1 = torch.mean(attn_outputs[i][l1].view(-1,128),dim=0)\n",
    "            entity_2 = torch.mean(attn_outputs[i][l2].view(-1,128),dim=0)\n",
    "            \n",
    "            linear_inputs[i][0] = entity_1\n",
    "            linear_inputs[i][1] = entity_2\n",
    "            \n",
    "        flatten_outputs = self.flatten(linear_inputs.to(self.device))\n",
    "        logits = self.linear(flatten_outputs)\n",
    "        \n",
    "        predictions = self.softmax(logits)\n",
    "        \n",
    "        return predictions\n",
    "        \n",
    "    def initialize_states(self,batch_size):\n",
    "        h0 = torch.zeros(2,batch_size,self.hidden_size).to(self.device)\n",
    "        c0 = torch.zeros(2,batch_size,self.hidden_size).to(self.device)\n",
    "        return h0,c0\n",
    "    \n",
    "    def calculate_qkv(self,hidden_states):\n",
    "        \n",
    "        q = torch.matmul(hidden_states,self.query_weights)\n",
    "        k = torch.matmul(hidden_states,self.key_weights)\n",
    "        v = torch.matmul(hidden_states,self.value_weights)\n",
    "        \n",
    "        return q,k,v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "354046d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-3db4d6c0d887>:50: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  predictions = self.softmax(logits)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.2870354652404785\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "loss: 2.2901110649108887\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "loss: 2.1240410804748535\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "loss: 2.0762276649475098\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "loss: 2.0600991249084473\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "loss: 2.0403311252593994\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "loss: 1.9960249662399292\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "loss: 1.9029656648635864\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "loss: 1.9577655792236328\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "loss: 1.997976541519165\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "loss: 1.9810819625854492\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "loss: 2.028693675994873\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "loss: 1.8929334878921509\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "loss: 1.936974287033081\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "loss: 1.959375262260437\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "loss: 2.0002286434173584\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "loss: 1.9727668762207031\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "loss: 1.896070957183838\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "loss: 1.9176459312438965\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "loss: 1.882678508758545\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "loss: 1.9108808040618896\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "loss: 1.8016083240509033\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "loss: 1.8796712160110474\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "loss: 1.8751388788223267\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "loss: 1.8511474132537842\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "loss: 1.8031176328659058\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "loss: 1.7763192653656006\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "loss: 1.8955973386764526\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "loss: 1.8276187181472778\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "loss: 1.8343802690505981\n",
      "------------------------------\n",
      "Epoch: 31\n",
      "loss: 1.7902781963348389\n",
      "------------------------------\n",
      "Epoch: 32\n",
      "loss: 1.6646562814712524\n",
      "------------------------------\n",
      "Epoch: 33\n",
      "loss: 1.788393497467041\n",
      "------------------------------\n",
      "Epoch: 34\n",
      "loss: 1.8169320821762085\n",
      "------------------------------\n",
      "Epoch: 35\n",
      "loss: 1.7208197116851807\n",
      "------------------------------\n",
      "Epoch: 36\n",
      "loss: 1.72257399559021\n",
      "------------------------------\n",
      "Epoch: 37\n",
      "loss: 1.6959528923034668\n",
      "------------------------------\n",
      "Epoch: 38\n",
      "loss: 1.7243375778198242\n",
      "------------------------------\n",
      "Epoch: 39\n",
      "loss: 1.7821455001831055\n",
      "------------------------------\n",
      "Epoch: 40\n",
      "loss: 1.7901878356933594\n",
      "------------------------------\n",
      "Epoch: 41\n",
      "loss: 1.756304144859314\n",
      "------------------------------\n",
      "Epoch: 42\n",
      "loss: 1.7273633480072021\n",
      "------------------------------\n",
      "Epoch: 43\n",
      "loss: 1.7913355827331543\n",
      "------------------------------\n",
      "Epoch: 44\n",
      "loss: 1.6636593341827393\n",
      "------------------------------\n",
      "Epoch: 45\n",
      "loss: 1.6649036407470703\n",
      "------------------------------\n",
      "Epoch: 46\n",
      "loss: 1.7152010202407837\n",
      "------------------------------\n",
      "Epoch: 47\n",
      "loss: 1.7333827018737793\n",
      "------------------------------\n",
      "Epoch: 48\n",
      "loss: 1.6638391017913818\n",
      "------------------------------\n",
      "Epoch: 49\n",
      "loss: 1.7111387252807617\n",
      "------------------------------\n",
      "Epoch: 50\n",
      "loss: 1.728105068206787\n",
      "------------------------------\n",
      "Epoch: 51\n",
      "loss: 1.691169261932373\n",
      "------------------------------\n",
      "Epoch: 52\n",
      "loss: 1.6465363502502441\n",
      "------------------------------\n",
      "Epoch: 53\n",
      "loss: 1.7130248546600342\n",
      "------------------------------\n",
      "Epoch: 54\n",
      "loss: 1.668931245803833\n",
      "------------------------------\n",
      "Epoch: 55\n",
      "loss: 1.671207070350647\n",
      "------------------------------\n",
      "Epoch: 56\n",
      "loss: 1.7294126749038696\n",
      "------------------------------\n",
      "Epoch: 57\n",
      "loss: 1.6594724655151367\n",
      "------------------------------\n",
      "Epoch: 58\n",
      "loss: 1.677122950553894\n",
      "------------------------------\n",
      "Epoch: 59\n",
      "loss: 1.6492812633514404\n",
      "------------------------------\n",
      "Epoch: 60\n",
      "loss: 1.6536625623703003\n",
      "------------------------------\n",
      "Epoch: 61\n",
      "loss: 1.6397099494934082\n",
      "------------------------------\n",
      "Epoch: 62\n",
      "loss: 1.4986169338226318\n",
      "------------------------------\n",
      "Epoch: 63\n",
      "loss: 1.6643908023834229\n",
      "------------------------------\n",
      "Epoch: 64\n",
      "loss: 1.6605339050292969\n",
      "------------------------------\n",
      "Epoch: 65\n",
      "loss: 1.6844491958618164\n",
      "------------------------------\n",
      "Epoch: 66\n",
      "loss: 1.6388962268829346\n",
      "------------------------------\n",
      "Epoch: 67\n",
      "loss: 1.6512744426727295\n",
      "------------------------------\n",
      "Epoch: 68\n",
      "loss: 1.5769312381744385\n",
      "------------------------------\n",
      "Epoch: 69\n",
      "loss: 1.5283966064453125\n",
      "------------------------------\n",
      "Epoch: 70\n",
      "loss: 1.6061512231826782\n",
      "------------------------------\n",
      "Epoch: 71\n",
      "loss: 1.6127371788024902\n",
      "------------------------------\n",
      "Epoch: 72\n",
      "loss: 1.6042983531951904\n",
      "------------------------------\n",
      "Epoch: 73\n",
      "loss: 1.5564794540405273\n",
      "------------------------------\n",
      "Epoch: 74\n",
      "loss: 1.5457639694213867\n",
      "------------------------------\n",
      "Epoch: 75\n",
      "loss: 1.5083235502243042\n",
      "------------------------------\n",
      "Epoch: 76\n",
      "loss: 1.5704255104064941\n",
      "------------------------------\n",
      "Epoch: 77\n",
      "loss: 1.5391271114349365\n",
      "------------------------------\n",
      "Epoch: 78\n",
      "loss: 1.5083415508270264\n",
      "------------------------------\n",
      "Epoch: 79\n",
      "loss: 1.521148443222046\n",
      "------------------------------\n",
      "Epoch: 80\n",
      "loss: 1.5854017734527588\n",
      "------------------------------\n",
      "Epoch: 81\n",
      "loss: 1.5860934257507324\n",
      "------------------------------\n",
      "Epoch: 82\n",
      "loss: 1.5860488414764404\n",
      "------------------------------\n",
      "Epoch: 83\n",
      "loss: 1.5541388988494873\n",
      "------------------------------\n",
      "Epoch: 84\n",
      "loss: 1.5545177459716797\n",
      "------------------------------\n",
      "Epoch: 85\n",
      "loss: 1.60099458694458\n",
      "------------------------------\n",
      "Epoch: 86\n",
      "loss: 1.5713682174682617\n",
      "------------------------------\n",
      "Epoch: 87\n",
      "loss: 1.6469147205352783\n",
      "------------------------------\n",
      "Epoch: 88\n",
      "loss: 1.5076353549957275\n",
      "------------------------------\n",
      "Epoch: 89\n",
      "loss: 1.5234887599945068\n",
      "------------------------------\n",
      "Epoch: 90\n",
      "loss: 1.5868868827819824\n",
      "------------------------------\n",
      "Epoch: 91\n",
      "loss: 1.539364218711853\n",
      "------------------------------\n",
      "Epoch: 92\n",
      "loss: 1.4612517356872559\n",
      "------------------------------\n",
      "Epoch: 93\n",
      "loss: 1.5698976516723633\n",
      "------------------------------\n",
      "Epoch: 94\n",
      "loss: 1.5993072986602783\n",
      "------------------------------\n",
      "Epoch: 95\n",
      "loss: 1.5380315780639648\n",
      "------------------------------\n",
      "Epoch: 96\n",
      "loss: 1.5697559118270874\n",
      "------------------------------\n",
      "Epoch: 97\n",
      "loss: 1.5227787494659424\n",
      "------------------------------\n",
      "Epoch: 98\n",
      "loss: 1.5690333843231201\n",
      "------------------------------\n",
      "Epoch: 99\n",
      "loss: 1.5392078161239624\n",
      "------------------------------\n",
      "Epoch: 100\n",
      "loss: 1.5381966829299927\n",
      "------------------------------\n",
      "Training is done!!\n"
     ]
    }
   ],
   "source": [
    "INPUT_SHAPE = 300\n",
    "HIDDEN_SHAPE = 128\n",
    "NUM_LAYERS = 1\n",
    "OUTPUT_SHAPE = 10\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(f'Using {device}')\n",
    "\n",
    "model = MyModel(INPUT_SHAPE,HIDDEN_SHAPE,NUM_LAYERS,OUTPUT_SHAPE,device).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "def train_one_epoch(model,dataloader,loss_fn,optimizer,device):\n",
    "    for i,(embeddings,positions,targets) in enumerate(dataloader):\n",
    "        embeddings,positions,targets = embeddings.to(device),positions.to(device),targets.to(device)\n",
    "        predictions = model((embeddings,positions))\n",
    "        loss = loss_fn(predictions,targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    print(f'loss: {loss.item()}')\n",
    "    \n",
    "def  train(model,dataloader,loss_fn,optimizer,device,epochs=100):\n",
    "    for i in range(epochs):\n",
    "        print(f\"Epoch: {i+1}\")\n",
    "        train_one_epoch(model,train_dataloader,loss_fn,optimizer,device)\n",
    "        print('-'*30)\n",
    "\n",
    "    print(\"Training is done!!\")\n",
    "train(model,train_dataloader,loss_fn,optimizer,device,epochs=100)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80ddf6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6, 8, 8, 1, 4, 3, 1, 7, 6, 3, 3, 3, 0, 8, 6, 2, 8, 6, 8, 2, 8, 4, 7, 8,\n",
      "        8, 9, 1, 1, 7, 8, 2, 0, 0, 2, 8, 7, 8, 7, 7, 8, 9, 8, 1, 3, 9, 6, 8, 4,\n",
      "        8, 7, 0, 8, 6, 4, 7, 8, 8, 8, 1, 3, 5, 4, 3, 0, 8, 0, 2, 2, 9, 7, 2, 6,\n",
      "        7, 8, 9, 0, 0, 9, 8, 8, 0, 6, 1, 6, 2, 8, 3, 5, 9, 2, 8, 8, 7, 8, 8, 7,\n",
      "        4, 2, 1, 1, 8, 0, 4, 7, 9, 4, 9, 9, 8, 0, 8, 8, 4, 8, 4, 3, 8, 7, 8, 1,\n",
      "        7, 4, 7, 3, 5, 7, 2, 2], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-3db4d6c0d887>:50: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  predictions = self.softmax(logits)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 7, 8, 6, 5, 8, 7, 0, 6, 5, 6, 1, 8, 7, 1, 8, 6, 4, 4, 7, 8, 2, 4, 9,\n",
      "        9, 8, 1, 2, 9, 0, 3, 1, 9, 8, 1, 1, 6, 8, 9, 0, 3, 7, 9, 6, 0, 0, 8, 8,\n",
      "        4, 6, 1, 7, 6, 3, 0, 8, 8, 7, 7, 7, 4, 1, 9, 0, 8, 9, 3, 3, 7, 3, 8, 7,\n",
      "        8, 2, 2, 7, 7, 1, 5, 0, 4, 0, 0, 7, 5, 0, 4, 6, 9, 0, 7, 8, 2, 7, 0, 0,\n",
      "        5, 2, 1, 9, 8, 5, 0, 4, 2, 4, 7, 3, 4, 5, 1, 8, 0, 8, 5, 2, 2, 8, 7, 4,\n",
      "        5, 0, 3, 1, 6, 0, 4, 3], device='cuda:0')\n",
      "tensor([2, 8, 8, 3, 3, 6, 4, 1, 7, 3, 6, 4, 7, 8, 2, 4, 3, 9, 3, 0, 4, 8, 3, 2,\n",
      "        7, 8, 1, 6, 7, 7, 8, 8, 8, 9, 0, 5, 9, 1, 8, 2, 7, 5, 2, 6, 0, 5, 8, 0,\n",
      "        9, 0, 7, 8, 9, 0, 7, 9, 8, 3, 9, 0, 7, 9, 3, 8, 1, 4, 8, 6, 1, 8, 2, 2,\n",
      "        8, 0, 1, 7, 7, 0, 1, 1, 6, 8, 5, 8, 2, 8, 0, 8, 3, 6, 7, 3, 8, 5, 6, 4,\n",
      "        3, 3, 0, 7, 8, 4, 7, 8, 5, 8, 7, 9, 4, 8, 3, 1, 0, 2, 7, 0, 7, 1, 4, 0,\n",
      "        3, 7, 6, 7, 1, 6, 9, 6], device='cuda:0')\n",
      "tensor([3, 0, 7, 3, 5, 0, 3, 2, 9, 0, 8, 8, 3, 3, 1, 1], device='cuda:0')\n",
      "Accuracy: 51.0000\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    n_samples = 0\n",
    "    n_correct = 0\n",
    "    for i,(embeddings,positions,targets) in enumerate(test_dataloader):\n",
    "        outputs = model((embeddings,positions))\n",
    "        outputs = outputs.argmax(dim=1)\n",
    "        print(outputs)\n",
    "        targets = targets.to(device)\n",
    "        targets = targets.argmax(dim=1)\n",
    "#         print(targets)\n",
    "        n_samples += targets.shape[0]\n",
    "        n_correct += (targets==outputs).sum().item()\n",
    "    total_acc = 100*(n_correct/n_samples)\n",
    "    print(f\"Accuracy: {total_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3433b60186a858f59969a5625bb11b2eef36c0afc3013542c8d125c2d5e92e66"
  },
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
