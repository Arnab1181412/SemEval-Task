{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09e43aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import spacy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff20f610",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path = 'dataset/dataset/SemEval2010_task8_training/TRAIN_FILE.TXT'\n",
    "with open(training_path,'r') as f:\n",
    "    text = f.readlines()    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3334eb37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the system as described above has its greatest application in an arrayed <e1>configuration</e1> of antenna <e2>elements</e2>.',\n",
       " 'the <e1>child</e1> was carefully wrapped and bound into the <e2>cradle</e2> by means of a cord.',\n",
       " 'the <e1>author</e1> of a keygen uses a <e2>disassembler</e2> to look at the raw assembly code.',\n",
       " 'a misty <e1>ridge</e1> uprises from the <e2>surge</e2>.',\n",
       " 'the <e1>student</e1> <e2>association</e2> is the voice of the undergraduate student population of the state university of new york at buffalo.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = []\n",
    "for index in range(0,8000,4):\n",
    "    text[index] = text[index].replace('\\t',' ')\n",
    "    text[index] = text[index].replace('\\\"','')\n",
    "    text[index] = text[index].replace('\\n','')\n",
    "    words = text[index].split()\n",
    "    text[index] = ' '.join(words[1:])\n",
    "    \n",
    "    sentences.append(text[index].lower())\n",
    "sentences[:5]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3edd5897",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-3161dcca0716>:40: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:210.)\n",
      "  pos_entity = torch.tensor(pos_entity)\n"
     ]
    }
   ],
   "source": [
    "pos_entity = []\n",
    "for i,sentence in enumerate(sentences):\n",
    "    \n",
    "    pos_e1_low = sentence.index('<e1>') + 4\n",
    "    pos_e1_high = sentence.index('</e1>')\n",
    "    e1 = sentence[pos_e1_low:pos_e1_high]\n",
    "    \n",
    "    pos_e2_low = sentence.index('<e2>') + 4\n",
    "    pos_e2_high = sentence.index('</e2>')\n",
    "    e2 = sentence[pos_e2_low:pos_e2_high]\n",
    "    \n",
    "    sentences[i] = re.sub('<e1>|</e1>|<e2>|</e2>',' ',sentence)\n",
    "    tokens = sentences[i].split()\n",
    "    sentences[i] = ' '.join(tokens)\n",
    "#     print(tokens)\n",
    "    e1_list = []\n",
    "    for entity in e1.split():\n",
    "        e1_list.append(tokens.index(entity))\n",
    "    e2_list = []\n",
    "    for entity in e2.split():\n",
    "        e2_list.append(tokens.index(entity))\n",
    "    \n",
    "    pos_entity.append([e1_list,e2_list])\n",
    "max_tokens = 0\n",
    "for element in pos_entity:\n",
    "#     print(element)\n",
    "    e1 = len(element[0])\n",
    "    e2 = len(element[1])\n",
    "    local_max = max(e1,e2)\n",
    "    max_tokens = max(max_tokens,local_max)\n",
    "for i,element in enumerate(pos_entity):\n",
    "    e1_len = max_tokens - len(element[0])\n",
    "    e1 = torch.tensor(element[0])\n",
    "    e1 = F.pad(e1,(0,e1_len),\"constant\",-1).numpy()\n",
    "    \n",
    "    e2_len = max_tokens - len(element[1])\n",
    "    e2 = torch.tensor(element[1])\n",
    "    e2 = F.pad(e2,(0,e2_len),\"constant\",-1).numpy()\n",
    "    pos_entity[i] = np.array([e1,e2])\n",
    "pos_entity = torch.tensor(pos_entity)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73673a58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Component-Whole(e2,e1)',\n",
       " 'Other-Relation',\n",
       " 'Other-Relation(e2,e1)',\n",
       " 'Other-Relation',\n",
       " 'Other-Relation(e1,e2)']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allowed_relations = ['Cause-Effect', 'Component-Whole', 'Entity-Destination','Entity-Origin','Other-Relation']\n",
    "relations = []\n",
    "for index in range(1,8000,4):\n",
    "    text[index] = text[index].replace('\\n','')\n",
    "    if text[index].find('(') != -1:\n",
    "        index_of_braces = text[index].find('(')\n",
    "        relation = text[index][:index_of_braces]\n",
    "        if relation not in allowed_relations:\n",
    "            relation = 'Other-Relation'\n",
    "            text[index] = relation + text[index][index_of_braces:]\n",
    "    else:\n",
    "        text[index] = 'Other-Relation'\n",
    "    relations.append(text[index])\n",
    "relations[:5]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86cfb9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "input_dataset = []\n",
    "max_len = max([len(sentence.split()) for sentence in sentences])\n",
    "for sentence in sentences:\n",
    "    doc = nlp(sentence)\n",
    "    vector = []\n",
    "    for token in doc:\n",
    "        vector.append(token.vector)\n",
    "    vector = torch.tensor(vector)\n",
    "    right_padding = max_len-vector.shape[0]\n",
    "    padding = nn.ZeroPad2d((0,right_padding,0,0))\n",
    "    vector = padding(vector.T)\n",
    "    input_dataset.append(vector.T.numpy())\n",
    "    \n",
    "input_dataset = torch.tensor(np.array(input_dataset),device='cuda:0')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa13b9a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2000, 5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "just_relation = []\n",
    "for relation in relations:\n",
    "    if relation.find('(')!=-1:\n",
    "        index = relation.find(\"(\")\n",
    "        relation = relation[:index]\n",
    "        just_relation.append(relation)\n",
    "    else:\n",
    "        just_relation.append(relation)\n",
    "just_relation = np.array(just_relation)        \n",
    "encoder = LabelEncoder()\n",
    "just_relation = torch.tensor(encoder.fit_transform(just_relation))\n",
    "ohe = F.one_hot(just_relation)\n",
    "ohe = ohe.to(dtype=torch.float)\n",
    "ohe.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df82c618",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data = input_dataset[:1600]\n",
    "        self.target = ohe[:1600]\n",
    "        self.position = pos_entity[:1600]\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        sample = self.data[index],self.position[index],self.target[index]\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data = input_dataset[1600:]\n",
    "        self.target = ohe[1600:]\n",
    "        self.position = pos_entity[1600:]\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        sample = self.data[index],self.position[index],self.target[index]\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d91dd826",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = TrainDataset()\n",
    "train_dataloader = DataLoader(train,batch_size=128,shuffle=True)\n",
    "test = TestDataset()\n",
    "test_dataloader = DataLoader(test,batch_size=128,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7631363f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self,input_shape,lstm_hidden_size,num_layers,output_shape,device):\n",
    "        super(MyModel,self).__init__()\n",
    "        self.input_shape = input_shape \n",
    "        self.hidden_size = lstm_hidden_size\n",
    "        self.output_shape = output_shape\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "        self.bidirectional_lstm = nn.LSTM(input_shape,lstm_hidden_size,num_layers,\n",
    "                                          bidirectional=True,batch_first=True)\n",
    "        E = self.hidden_size\n",
    "        w = torch.empty(E,E)\n",
    "        self.query_weights = nn.init.xavier_uniform_(w).to(device)\n",
    "        self.key_weights = nn.init.xavier_uniform_(w).to(device)\n",
    "        self.value_weights = nn.init.xavier_uniform_(w).to(device)\n",
    "        self.attention = nn.MultiheadAttention(lstm_hidden_size,num_heads=1,batch_first=True)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(2*lstm_hidden_size,output_shape)\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        h0,c0 = self.initialize_states(inputs[0].shape[0])\n",
    "        lstm_outputs,_ = self.bidirectional_lstm(inputs[0],(h0,c0))\n",
    "        lstm_outputs = torch.mean(lstm_outputs.view(lstm_outputs.shape[0],-1,2,self.hidden_size),dim=2)\n",
    "        query,key,value = self.calculate_qkv(lstm_outputs)\n",
    "        attn_outputs,_ = self.attention(query,key,value)\n",
    "        \n",
    "        batch_size = attn_outputs.shape[0]\n",
    "        linear_inputs = torch.empty(batch_size,2,attn_outputs.shape[-1])\n",
    "#         print(attn_outputs.shape)\n",
    "        for i in range(batch_size):\n",
    "            l1 = []\n",
    "            for j in inputs[1][i][0]:\n",
    "                if j != -1:\n",
    "                    l1.append(j.item())\n",
    "            l2 = []\n",
    "            for k in inputs[1][i][1]:\n",
    "                if k != -1:\n",
    "                    l2.append(k.item())\n",
    "#             print(attn_outputs[i][l1].shape)\n",
    "            entity_1 = torch.mean(attn_outputs[i][l1].view(-1,128),dim=0)\n",
    "            entity_2 = torch.mean(attn_outputs[i][l2].view(-1,128),dim=0)\n",
    "            \n",
    "            linear_inputs[i][0] = entity_1\n",
    "            linear_inputs[i][1] = entity_2\n",
    "            \n",
    "        flatten_outputs = self.flatten(linear_inputs.to(self.device))\n",
    "        logits = self.linear(flatten_outputs)\n",
    "        \n",
    "        predictions = self.softmax(logits)\n",
    "        \n",
    "        return predictions\n",
    "        \n",
    "    def initialize_states(self,batch_size):\n",
    "        h0 = torch.zeros(2,batch_size,self.hidden_size).to(self.device)\n",
    "        c0 = torch.zeros(2,batch_size,self.hidden_size).to(self.device)\n",
    "        return h0,c0\n",
    "    \n",
    "    def calculate_qkv(self,hidden_states):\n",
    "        \n",
    "        q = torch.matmul(hidden_states,self.query_weights)\n",
    "        k = torch.matmul(hidden_states,self.key_weights)\n",
    "        v = torch.matmul(hidden_states,self.value_weights)\n",
    "        \n",
    "        return q,k,v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "354046d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-3db4d6c0d887>:50: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  predictions = self.softmax(logits)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.3724713325500488\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "loss: 1.3423326015472412\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "loss: 1.2642076015472412\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "loss: 1.3735824823379517\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "loss: 1.4048324823379517\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "loss: 1.4204576015472412\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "loss: 1.1860826015472412\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "loss: 1.2798326015472412\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "loss: 1.3892076015472412\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "loss: 1.2642076015472412\n",
      "------------------------------\n",
      "Epoch: 11\n",
      "loss: 1.4517076015472412\n",
      "------------------------------\n",
      "Epoch: 12\n",
      "loss: 1.4048326015472412\n",
      "------------------------------\n",
      "Epoch: 13\n",
      "loss: 1.2798326015472412\n",
      "------------------------------\n",
      "Epoch: 14\n",
      "loss: 1.4829576015472412\n",
      "------------------------------\n",
      "Epoch: 15\n",
      "loss: 1.3892074823379517\n",
      "------------------------------\n",
      "Epoch: 16\n",
      "loss: 1.3110826015472412\n",
      "------------------------------\n",
      "Epoch: 17\n",
      "loss: 1.3110826015472412\n",
      "------------------------------\n",
      "Epoch: 18\n",
      "loss: 1.2798326015472412\n",
      "------------------------------\n",
      "Epoch: 19\n",
      "loss: 1.4673326015472412\n",
      "------------------------------\n",
      "Epoch: 20\n",
      "loss: 1.4360826015472412\n",
      "------------------------------\n",
      "Epoch: 21\n",
      "loss: 1.3735826015472412\n",
      "------------------------------\n",
      "Epoch: 22\n",
      "loss: 1.4517076015472412\n",
      "------------------------------\n",
      "Epoch: 23\n",
      "loss: 1.3423326015472412\n",
      "------------------------------\n",
      "Epoch: 24\n",
      "loss: 1.3267076015472412\n",
      "------------------------------\n",
      "Epoch: 25\n",
      "loss: 1.4048326015472412\n",
      "------------------------------\n",
      "Epoch: 26\n",
      "loss: 1.4204576015472412\n",
      "------------------------------\n",
      "Epoch: 27\n",
      "loss: 1.3735826015472412\n",
      "------------------------------\n",
      "Epoch: 28\n",
      "loss: 1.3579574823379517\n",
      "------------------------------\n",
      "Epoch: 29\n",
      "loss: 1.4048326015472412\n",
      "------------------------------\n",
      "Epoch: 30\n",
      "loss: 1.4048324823379517\n",
      "------------------------------\n",
      "Epoch: 31\n",
      "loss: 1.2485826015472412\n",
      "------------------------------\n",
      "Epoch: 32\n",
      "loss: 1.3892076015472412\n",
      "------------------------------\n",
      "Epoch: 33\n",
      "loss: 1.3892076015472412\n",
      "------------------------------\n",
      "Epoch: 34\n",
      "loss: 1.2798326015472412\n",
      "------------------------------\n",
      "Epoch: 35\n",
      "loss: 1.2329576015472412\n",
      "------------------------------\n",
      "Epoch: 36\n",
      "loss: 1.3892076015472412\n",
      "------------------------------\n",
      "Epoch: 37\n",
      "loss: 1.2954574823379517\n",
      "------------------------------\n",
      "Epoch: 38\n",
      "loss: 1.4204576015472412\n",
      "------------------------------\n",
      "Epoch: 39\n",
      "loss: 1.3423326015472412\n",
      "------------------------------\n",
      "Epoch: 40\n",
      "loss: 1.4048326015472412\n",
      "------------------------------\n",
      "Epoch: 41\n",
      "loss: 1.3735826015472412\n",
      "------------------------------\n",
      "Epoch: 42\n",
      "loss: 1.5142076015472412\n",
      "------------------------------\n",
      "Epoch: 43\n",
      "loss: 1.4204574823379517\n",
      "------------------------------\n",
      "Epoch: 44\n",
      "loss: 1.4673326015472412\n",
      "------------------------------\n",
      "Epoch: 45\n",
      "loss: 1.2798324823379517\n",
      "------------------------------\n",
      "Epoch: 46\n",
      "loss: 1.3892076015472412\n",
      "------------------------------\n",
      "Epoch: 47\n",
      "loss: 1.3267074823379517\n",
      "------------------------------\n",
      "Epoch: 48\n",
      "loss: 1.2329576015472412\n",
      "------------------------------\n",
      "Epoch: 49\n",
      "loss: 1.3423326015472412\n",
      "------------------------------\n",
      "Epoch: 50\n",
      "loss: 1.3267076015472412\n",
      "------------------------------\n",
      "Epoch: 51\n",
      "loss: 1.2485826015472412\n",
      "------------------------------\n",
      "Epoch: 52\n",
      "loss: 1.3267076015472412\n",
      "------------------------------\n",
      "Epoch: 53\n",
      "loss: 1.3267076015472412\n",
      "------------------------------\n",
      "Epoch: 54\n",
      "loss: 1.3892076015472412\n",
      "------------------------------\n",
      "Epoch: 55\n",
      "loss: 1.2954576015472412\n",
      "------------------------------\n",
      "Epoch: 56\n",
      "loss: 1.4517076015472412\n",
      "------------------------------\n",
      "Epoch: 57\n",
      "loss: 1.3892076015472412\n",
      "------------------------------\n",
      "Epoch: 58\n",
      "loss: 1.2954576015472412\n",
      "------------------------------\n",
      "Epoch: 59\n",
      "loss: 1.3423326015472412\n",
      "------------------------------\n",
      "Epoch: 60\n",
      "loss: 1.3579576015472412\n",
      "------------------------------\n",
      "Epoch: 61\n",
      "loss: 1.4204574823379517\n",
      "------------------------------\n",
      "Epoch: 62\n",
      "loss: 1.3110826015472412\n",
      "------------------------------\n",
      "Epoch: 63\n",
      "loss: 1.4048326015472412\n",
      "------------------------------\n",
      "Epoch: 64\n",
      "loss: 1.3579576015472412\n",
      "------------------------------\n",
      "Epoch: 65\n",
      "loss: 1.4204576015472412\n",
      "------------------------------\n",
      "Epoch: 66\n",
      "loss: 1.4204574823379517\n",
      "------------------------------\n",
      "Epoch: 67\n",
      "loss: 1.4829574823379517\n",
      "------------------------------\n",
      "Epoch: 68\n",
      "loss: 1.2954576015472412\n",
      "------------------------------\n",
      "Epoch: 69\n",
      "loss: 1.3110826015472412\n",
      "------------------------------\n",
      "Epoch: 70\n",
      "loss: 1.2798326015472412\n",
      "------------------------------\n",
      "Epoch: 71\n",
      "loss: 1.2329576015472412\n",
      "------------------------------\n",
      "Epoch: 72\n",
      "loss: 1.3423326015472412\n",
      "------------------------------\n",
      "Epoch: 73\n",
      "loss: 1.3735826015472412\n",
      "------------------------------\n",
      "Epoch: 74\n",
      "loss: 1.3579576015472412\n",
      "------------------------------\n",
      "Epoch: 75\n",
      "loss: 1.3110826015472412\n",
      "------------------------------\n",
      "Epoch: 76\n",
      "loss: 1.3267076015472412\n",
      "------------------------------\n",
      "Epoch: 77\n",
      "loss: 1.3110826015472412\n",
      "------------------------------\n",
      "Epoch: 78\n",
      "loss: 1.3579576015472412\n",
      "------------------------------\n",
      "Epoch: 79\n",
      "loss: 1.3423326015472412\n",
      "------------------------------\n",
      "Epoch: 80\n",
      "loss: 1.2798324823379517\n",
      "------------------------------\n",
      "Epoch: 81\n",
      "loss: 1.4360826015472412\n",
      "------------------------------\n",
      "Epoch: 82\n",
      "loss: 1.2485826015472412\n",
      "------------------------------\n",
      "Epoch: 83\n",
      "loss: 1.2954576015472412\n",
      "------------------------------\n",
      "Epoch: 84\n",
      "loss: 1.3892076015472412\n",
      "------------------------------\n",
      "Epoch: 85\n",
      "loss: 1.3110824823379517\n",
      "------------------------------\n",
      "Epoch: 86\n",
      "loss: 1.3267076015472412\n",
      "------------------------------\n",
      "Epoch: 87\n",
      "loss: 1.4360826015472412\n",
      "------------------------------\n",
      "Epoch: 88\n",
      "loss: 1.3110824823379517\n",
      "------------------------------\n",
      "Epoch: 89\n",
      "loss: 1.4204576015472412\n",
      "------------------------------\n",
      "Epoch: 90\n",
      "loss: 1.2798326015472412\n",
      "------------------------------\n",
      "Epoch: 91\n",
      "loss: 1.3423326015472412\n",
      "------------------------------\n",
      "Epoch: 92\n",
      "loss: 1.3267076015472412\n",
      "------------------------------\n",
      "Epoch: 93\n",
      "loss: 1.3579574823379517\n",
      "------------------------------\n",
      "Epoch: 94\n",
      "loss: 1.2798326015472412\n",
      "------------------------------\n",
      "Epoch: 95\n",
      "loss: 1.4517076015472412\n",
      "------------------------------\n",
      "Epoch: 96\n",
      "loss: 1.4829574823379517\n",
      "------------------------------\n",
      "Epoch: 97\n",
      "loss: 1.3735826015472412\n",
      "------------------------------\n",
      "Epoch: 98\n",
      "loss: 1.2642076015472412\n",
      "------------------------------\n",
      "Epoch: 99\n",
      "loss: 1.3110826015472412\n",
      "------------------------------\n",
      "Epoch: 100\n",
      "loss: 1.2642074823379517\n",
      "------------------------------\n",
      "Training is done!!\n"
     ]
    }
   ],
   "source": [
    "INPUT_SHAPE = 300\n",
    "HIDDEN_SHAPE = 128\n",
    "NUM_LAYERS = 1\n",
    "OUTPUT_SHAPE = 5\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(f'Using {device}')\n",
    "\n",
    "model = MyModel(INPUT_SHAPE,HIDDEN_SHAPE,NUM_LAYERS,OUTPUT_SHAPE,device).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "def train_one_epoch(model,dataloader,loss_fn,optimizer,device):\n",
    "    for i,(embeddings,positions,targets) in enumerate(dataloader):\n",
    "        embeddings,positions,targets = embeddings.to(device),positions.to(device),targets.to(device)\n",
    "        predictions = model((embeddings,positions))\n",
    "        loss = loss_fn(predictions,targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    print(f'loss: {loss.item()}')\n",
    "    \n",
    "def  train(model,dataloader,loss_fn,optimizer,device,epochs=100):\n",
    "    for i in range(epochs):\n",
    "        print(f\"Epoch: {i+1}\")\n",
    "        train_one_epoch(model,train_dataloader,loss_fn,optimizer,device)\n",
    "        print('-'*30)\n",
    "\n",
    "    print(\"Training is done!!\")\n",
    "train(model,train_dataloader,loss_fn,optimizer,device,epochs=100)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80ddf6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-3db4d6c0d887>:50: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  predictions = self.softmax(logits)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4], device='cuda:0')\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4], device='cuda:0')\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4], device='cuda:0')\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], device='cuda:0')\n",
      "Accuracy: 56.5000\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    n_samples = 0\n",
    "    n_correct = 0\n",
    "    for i,(embeddings,positions,targets) in enumerate(test_dataloader):\n",
    "        outputs = model((embeddings,positions))\n",
    "        outputs = outputs.argmax(dim=1)\n",
    "        print(outputs)\n",
    "        targets = targets.to(device)\n",
    "        targets = targets.argmax(dim=1)\n",
    "#         print(targets)\n",
    "        n_samples += targets.shape[0]\n",
    "        n_correct += (targets==outputs).sum().item()\n",
    "    total_acc = 100*(n_correct/n_samples)\n",
    "    print(f\"Accuracy: {total_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd49d04f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3433b60186a858f59969a5625bb11b2eef36c0afc3013542c8d125c2d5e92e66"
  },
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
